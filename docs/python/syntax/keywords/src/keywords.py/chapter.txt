! CONTROL:
	/type/
		module
	/element/
		- (control)`property-set`
		- (source/area)`1 1 466 38`
Programming language profile and parser class for Keyword Oriented Syntax.

Provides a data structure for describing keyword based languages that allows
a naive processor to assign classifications to the tokens contained
by a string.

(factor-local/class)&<#Profile[Profile]> instances have limited information regarding languages and (factor-local/class)&<#Parser[Parser]> instances should
have naive processing algorithms. This intentional deficiency is a product of the goal to
keep Keywords based interpretations trivial to implement and their parameter set small so that the
the profile may be quickly and easily defined by users without requiring volumes of documentation
to be consumed.

[ Parser Process Methods ]

(factor-local/class)&<#Parser[Parser]> instances have two high-level methods for producing tokens: (factor-local/method)&<#Parser.process_lines[Parser.process_lines]> and
(factor-local/method)&<#Parser.process_document[Parser.process_document]>. The former resets the context provided to (factor-local/method)&<#Parser.delimit[Parser.delimit]> for each line,
and the latter maintains it throughout the lifetime of the generator.

While (factor-local/method)&<#Parser.process_document[Parser.process_document]> appears desireable in many cases, the limitations of (factor-local/class)&<#Profile[Profile]> instances
may make it reasonable to choose (factor-local/method)&<#Parser.process_lines[Parser.process_lines]> in order to avoid the effects of an
inaccruate profile or a language that maintains ambiguities with respect to the parser's capabilities.

[ Engineering ]

Essentially, this is a lexer whose tokens are defined by (factor-local/class)&<#Profile[Profile]> instances.
The language types that are matches for applications are usually keyword based
and leverage whitespace for isolation of fields.


[ typing ]
! CONTROL:
	/type/
		import
	/flags/
		- `undocumented`
	/element/
		- (control)`property-set`
		- (source/area)`30 1 30 13`

[ itertools ]
! CONTROL:
	/type/
		import
	/flags/
		- `undocumented`
	/element/
		- (control)`property-set`
		- (source/area)`31 1 31 16`

[ functools ]
! CONTROL:
	/type/
		import
	/flags/
		- `undocumented`
	/element/
		- (control)`property-set`
		- (source/area)`32 1 32 16`

[ string ]
! CONTROL:
	/type/
		import
	/flags/
		- `undocumented`
	/element/
		- (control)`property-set`
		- (source/area)`34 1 34 28`

[ Tokens ]
! CONTROL:
	/type/
		data
	/flags/
		- `undocumented`
	/element/
		- (control)`property-set`
		- (source/area)`36 1 36 51`
#!source
	Tokens = typing.Iterable[typing.Tuple[str,str,str]]

[ Profile ]
! CONTROL:
	/type/
		class
	/element/
		- (control)`property-set`
		- (source/area)`38 1 170 37`
! INHERIT:
	- (control)`property-set`
	- (source/area)`38 15 38 19`
	- (type/syntax)`tuple`
	- (type/invalid/unknown)&<#tuple>
	
Data structure describing the elements of a Keyword Oriented Syntax.

Empty strings present in any of these sets will usually refer to the End of Line.
This notation is primarily intended for area exclusions for supporting line comments,
but (factor-local/property)&<#Profile.literals[literals]> and (factor-local/property)&<#Profile.enclosures[enclosures]> may also use them to represent the beginning or end of a line.

While (factor-local/class)&<#Profile[Profile]> is a tuple subclass, indexes should *not* be used to access members.


[ Profile >> __slots__ ]
! CONTROL:
	/type/
		data
	/flags/
		- `undocumented`
	/element/
		- (control)`property-set`
		- (source/area)`48 2 50 13`
#!source
	__slots__ = ()
		@classmethod

[ Profile >> from_keywords_v1 ]
! CONTROL:
	/type/
		classmethod
	/flags/
		- `undocumented`
	/element/
		- (control)`property-set`
		- (source/area)`51 2 71 10`
(signature)`from_keywords_v1(Class, **wordtypes)`


[ Profile >> words ]
! CONTROL:
	/type/
		property
	/element/
		- (control)`property-set`
		- (source/area)`72 2 78 10`
		- (type/syntax)`typing.Mapping[str, typing.Set[str]]`
		
Dictionary associating sets of identifier strings with a classification identifier.


[ Profile >> exclusions ]
! CONTROL:
	/type/
		property
	/element/
		- (control)`property-set`
		- (source/area)`79 2 87 10`
		- (type/syntax)`typing.Set[typing.Tuple[str,str]]`
		
Comment start and stop pairs delimiting an excluded area from the source.

Exclusions are given the second highest priority by (factor-local/class)&<#Parser[Parser]>.


[ Profile >> literals ]
! CONTROL:
	/type/
		property
	/element/
		- (control)`property-set`
		- (source/area)`88 2 98 10`
		- (type/syntax)`typing.Set[typing.Tuple[str,str]]`
		
The start and stop pairs delimiting a literal area within the syntax document.
Primarily used for string quotations, but supports distinct stops for
handling other cases as well.

Literals are given the highest priority by (factor-local/class)&<#Parser[Parser]>.


[ Profile >> enclosures ]
! CONTROL:
	/type/
		property
	/element/
		- (control)`property-set`
		- (source/area)`99 2 107 10`
		- (type/syntax)`typing.Set[typing.Tuple[str,str]]`
		
The start and stop pairs delimiting an expression.

Enclosures have the highest precedence during expression processing.


[ Profile >> routers ]
! CONTROL:
	/type/
		property
	/element/
		- (control)`property-set`
		- (source/area)`108 2 114 10`
		- (type/syntax)`typing.Set[str]`
		
Operators used to designate a resolution path for selecting an object to be used.


[ Profile >> operations ]
! CONTROL:
	/type/
		property
	/element/
		- (control)`property-set`
		- (source/area)`115 2 122 10`
		- (type/syntax)`typing.Set[str]`
		
Set of operators that can perform some manipulation to the objects associated
with the adjacent identifiers.


[ Profile >> terminators ]
! CONTROL:
	/type/
		property
	/element/
		- (control)`property-set`
		- (source/area)`123 2 129 10`
		- (type/syntax)`typing.Set[str]`
		
Operators used to designate the end of a statement, expression, or field.


[ Profile >> operators ]
! CONTROL:
	/type/
		property
	/element/
		- (control)`property-set`
		- (source/area)`130 2 170 37`
		- (type/syntax)`typing.Iterable[str]`
		
Emit all unit operators employed by the language associated with a rank and context effect.
Operators may appears multiple times. Empty strings represent end of line.

Operators are emitted by their classification in the following order:

# Operations
# Routers
# Terminators
# Enclosures
# Literals
# Exclusions

Order is deliberate in order to allow mappings to be directly built so
later classes will overwrite earlier entries in cases of ambiguity.


[ Parser ]
! CONTROL:
	/type/
		class
	/element/
		- (control)`property-set`
		- (source/area)`172 1 467 37`
! INHERIT:
	- (control)`property-set`
	- (source/area)`172 14 172 19`
	- (type/syntax)`object`
	- (type/invalid/unknown)&<#object>
	
Keyword Oriented Syntax parser providing tokenization and region delimiting.

Instances do not hold state and methods of the same instance may be used
by multiple threads.

[ > Engineering ]

This is essentially a tightly coupled partial application for (factor-local/method)&<#Parser.tokenize[tokenize]>
and (factor-local/method)&<#Parser.delimit[delimit]>. (factor-local/classmethod)&<#Parser.from_profile[from_profile]> builds necessary parameters using a (factor-local/class)&<#Profile[Profile]>
instance and the internal constructor, (factor-local/method)&<#Parser.__init__[__init__]>, makes them available to the methods.

Applications should create and cache an instance for a given language.


[ Parser >> from_profile ]
! CONTROL:
	/type/
		classmethod
	/element/
		- (control)`property-set`
		- (source/area)`189 2 254 3`
(signature)`from_profile(Class, profile)`

Primary constructor for (factor-local/class)&<#Parser[Parser]>.

Instances should usually be cached when repeat use is expected as
some amount of preparation is performed by (factor-local/classmethod)&<#Parser.from_profile[from_profile]>.


[ Parser >> __init__ ]
! CONTROL:
	/type/
		method
	/element/
		- (control)`property-set`
		- (source/area)`256 2 280 82`
(signature)`__init__(self, profile, opset, opmap, delimiter, optable, exits, classify_id, classify_op)`

! WARNING: Do not use directly.
	The initializer's parameters are subject to change.
	(factor-local/classmethod)&<#Parser.from_profile[from_profile]> should be used to build instances.


[ Parser >> process_lines ]
! CONTROL:
	/type/
		method
	/element/
		- (control)`property-set`
		- (source/area)`282 2 297 59`
		- (type/syntax)`typing.Iterable[typing.Iterable[Tokens]]`
		
(signature)`process_lines(self, lines)`

Process lines using context resets;
(factor-local/method)&<#Parser.tokenize[tokenize]> and (factor-local/method)&<#Parser.delimit[delimit]> multiple (factor-local/parameter)&<#Parser.process_lines.Parameters.lines[lines]> resetting the context at the end of each line.

This is the recommended method for extracting tokens from a file for syntax documents
that are expected to restate line context, have inaccurate profiles, or are incomplete.

The produced iterators may be ran out of order as no parsing state is shared across lines.


[ Parser >> process_document ]
! CONTROL:
	/type/
		method
	/element/
		- (control)`property-set`
		- (source/area)`299 2 316 41`
		- (type/syntax)`typing.Iterable[typing.Iterable[Tokens]]`
		
(signature)`process_document(self, lines)`

Process lines of a complete source code file using continuous context;
(factor-local/method)&<#Parser.tokenize[tokenize]> and (factor-local/method)&<#Parser.delimit[delimit]> multiple lines maintaining the context across all (factor-local/parameter)&<#Parser.process_document.Parameters.lines[lines]>.

This is the recommended method for extracting tokens from a file for syntax documents
that are expected to *not* restate line context *and* have accurate profiles.

The produced iterators **must** be ran in the produced order as the context is shared across
instances.


[ Parser >> allocstack ]
! CONTROL:
	/type/
		method
	/element/
		- (control)`property-set`
		- (source/area)`318 2 322 30`
(signature)`allocstack(self)`

Allocate context stack for use with (factor-local/method)&<#Parser.delimit[delimit]>.


[ Parser >> delimit ]
! CONTROL:
	/type/
		method
	/element/
		- (control)`property-set`
		- (source/area)`324 2 399 15`
		- (type/syntax)`Tokens`
		- (type/factor-local/data)&<#Tokens[Tokens]>
		
(signature)`delimit(self, context, tokens)`

Insert switch tokens into an iteration of tokens marking the
boundaries of expressions, comments and quotations.

(factor-local/parameter)&<#Parser.delimit.Parameters.context[context]> is manipulated during the iteration and maintains the
nested state of comments. (factor-local/method)&<#Parser.allocstack[allocstack]> may be used to allocate an
initial state.

This is a relatively low-level method; (factor-local/method)&<#Parser.process_lines[process_lines]> or (factor-local/method)&<#Parser.process_document[process_document]>
should normally be used.


[ Parser >> tokenize ]
! CONTROL:
	/type/
		method
	/element/
		- (control)`property-set`
		- (source/area)`401 2 467 37`
		- (type/syntax)`Tokens`
		- (type/factor-local/data)&<#Tokens[Tokens]>
		
(signature)`tokenize(self, line)`

Tokenize a string of syntax according to the profile.

Direct use of this is not recommended as boundaries are not signalled.
(factor-local/method)&<#Parser.process_lines[process_lines]> or (factor-local/method)&<#Parser.process_document[process_document]> should be used.
The raw tokens, however, are usable in contexts where boundary information is
not desired or is not accurate enough for an application's use.

